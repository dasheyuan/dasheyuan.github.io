<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>机器学习笔记5 神经网络1 模型表达 - Chen Yuan&#39;s Blogs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="keywords" content="">
  <link rel="canonical" href="http://dasheyuan.com/post/2015-03-03-2">

  
  

  
  

  
  

  <link rel="stylesheet" type="text/css" href="http://dasheyuan.com/css/combined-min.css">

</head>
<body class="">

<div class="site-wrap">
  <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://dasheyuan.com" class="site-title">Chen Yuan&#39;s Blogs</a>
      <nav class="site-nav right">
      <a href="/post">日志</a>
<a href="/tags">标签</a>
<a href="/about">关于</a>

</form>

      </nav>
      <div class="clearfix"></div>
    </div>
  </div>
</header>

  <div class="post p2 p-responsive wrap" role="main">
    <div class="measure">
      <div class="post-header mb2">
        <h3 class="py2">机器学习笔记5 神经网络1 模型表达</h3>
        <span class="post-meta">Mar 3, 2015 by </span><br>
        
      </div>
       <p class="post-meta">Tags:&nbsp;
        
            
            <a href="http://dasheyuan.com/tags/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0">机器学习</a>
        
            ,&nbsp;
            <a href="http://dasheyuan.com/tags/%e6%9c%89%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0">有监督学习</a>
        
            ,&nbsp;
            <a href="http://dasheyuan.com/tags/%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c">神经网络</a>
        
      </p> 
      <article class="post-content">
      

<p>Andrew Ng cs229 Machine Learning 笔记</p>

<h1 id="神经网络:d35c8128725ed53542064246ed42d2c0">神经网络</h1>

<h2 id="非线性假设:d35c8128725ed53542064246ed42d2c0">非线性假设</h2>

<p>在特征变量数较大的情况下，采用线性回归会很难处理，比如我的数据集有3个特征变量，想要在假设中引入所有特征变量的平方项：</p>

<div>
$$g(\theta_0 + \theta_1x_1^2 + \theta_2x_1x_2 + \theta_3x_1x_3  + \theta_4x_2^2 + \theta_5x_2x_3  + \theta_6x_3^2 )$$
</div>

<p>共有6个特征，假设我们想知道选取其中任意两个可重复的平方项有多少组合，采用允许重复的组合公式计算$\frac{(n+r-1)!}{r!(n-1)!}$，共有$\frac{(3 + 2 - 1)!}{(2!\cdot (3-1)!)} = 6$种特征变量的组合。对于100个特征变量，则共有$\frac{(100 + 2 - 1)!}{(2\cdot (100-1)!)} = 5050$个新的特征变量。</p>

<p>可以大致估计特征变量的平方项组合个数的增长速度为$\mathcal{O}(\frac{n^2}2)$，立方项的组合个数的增长为$\mathcal{O}(n^3)$。这些增长都十分陡峭，让实际问题变得很棘手。</p>

<p>在变量假设十分复杂的情况下，神经网络提供了另一种机器学习算法。</p>

<h1 id="神经元和大脑:d35c8128725ed53542064246ed42d2c0">神经元和大脑</h1>

<p>神经网络是对大脑工作方式的一种简单模拟。有证据表明，大脑对所有的功能（如视觉，触觉，听觉等）都采用了一种“学习算法”。将听觉皮层和视觉神经连接到一起，听觉皮层可以学会“看见”。这种理论叫作“neuroplasticity”，已经得到了很多例子和实验验证。</p>

<h1 id="模型表达:d35c8128725ed53542064246ed42d2c0">模型表达</h1>

<p>简单来说，每个神经元都有输入（树突dendrites）和输出（轴突axons）。在模型中，输入就是我们的特征变量，输出就是模型假设的结果。</p>

<p>在神经网络中，分类问题通常采用logistic函数，也叫做sigmoid激活函数(sigmoid activation function)。$\theta$参数有时也被称为权重(weights)。</p>

<p>下面是一种简单的神经网络：</p>



<p>第一层是输入节点(nodes)，第二层是输出节点，也就是我们假设函数的结果$h_{\theta}(x)$。</p>

<p>第一层叫作“输入层”(input layer)，最后一层叫作“输出层”(output layer)，输入层和输出层之间还可以有多层，统称为“隐藏层”(hidden layer)。如下图中，第二层就叫隐藏层。隐藏层节点表示为$a^2_0 \cdots a^2_n$，被称作“激活单元(activation units)”。</p>



<p>$a_i^{(j)}$表示第$j$层的$i$单元被“激活”，$\Theta^{(j)}$表示从第$j$层到第$j+1$层的权重矩阵。</p>

<p>上图的神经网络中，激活单元的计算分别为：</p>

<div>
$$
\begin{align*}
a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline
a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline
a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline
h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline
\end{align*}
$$
</div>

<p>每一层都有自己的权重矩阵$\Theta^{(j)}$，如果第$j$层有 $s_j$ 个单元，第$j+1$层有$s_{j+1}$个单元，则$\Theta^{(j)}$是$s_{j+1} \times (s_j+1)$的矩阵。&rdquo;+1&rdquo;是因为第$j$层包括一个&rdquo;偏差节点(bias nodes)“，$x_0$和$\Theta_0^{(j)}$。换句话说，输出节点不包括偏差节点，但输入节点会包括偏差节点。</p>

<p>举个例子，第一层有2个输入节点，第二层有4个激活单元。$\Theta^{(1)}$的维度为$4 \times 3$。</p>

<p>下面将以上模型表达向量化。</p>

<p>采用$z^{(i)}_k$表示$g$函数的输入，那么有：</p>

<div>
$$
\begin{align*}
a_1^{(2)} = g(z_1^{(2)}) \newline
a_2^{(2)} = g(z_2^{(2)}) \newline
a_3^{(2)} = g(z_3^{(2)}) \newline
\end{align*}
$$
</div>

<p>给定第$j$层的节点$k$，变量$z$等于：</p>

<div>
$$
z_k^{(j)} = \Theta_{k,0}^{(j-1)}x_0 + \Theta_{k,1}^{(j-1)}x_1 + \cdots + \Theta_{k,n}^{(j-1)}x_n 
$$
<div>

$x$和$z^{(j)}$的向量表示为：

<div>
$$
\begin{align*}
x = 
\begin{bmatrix}
x_0 \newline
x_1 \newline
\cdots \newline
x_n
\end{bmatrix} &
z^{(j)} = 
\begin{bmatrix}
z_1^{(j)} \newline
z_2^{(j)} \newline
\cdots \newline
z_n^{(j)}
\end{bmatrix}
\end{align*}
$$
</div>

<p>因此，$z^{(j}) = \Theta^{(j-1)} x$</p>

<p>令$x = a^{(j-1)}$，则$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$</p>

<p>最后的结果为</p>

<div>
$$
h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) = g(\Theta^{(j)}a^{(j)})
$$
</div>

<h1 id="神经网络拟合逻辑运算:d35c8128725ed53542064246ed42d2c0">神经网络拟合逻辑运算</h1>

<ol>
<li>$x_1\ AND\ x_2$</li>
</ol>

<p>神经网络模型：</p>

<div>
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2
\end{bmatrix} \rightarrow
\begin{bmatrix}
g(z^{(2)})
\end{bmatrix} \rightarrow
h_\Theta(x)
\end{align*}
$$
</div>

<p>其中，$x_0$为偏差，恒等于1。</p>

<p>权重参数为：</p>

<div>
$\Theta^{(1)} = 
\begin{bmatrix}
-30 & 20 & 20
\end{bmatrix}$
</div>

<p>仅当$x_1$和$x_2$同时为1时，$h_{\Theta}(x) = 1$。</p>

<div>
\begin{align*}
& h_\Theta(x) = g(-30 + 20x_1 + 20x_2) \newline
\newline
& x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \newline
& x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \newline
& x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \newline
& x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1
\end{align*}
</div>

<ol>
<li>$x_1\ NOR\ x_2$, $NOR$为$NOT\ OR$</li>
</ol>

<p>模型同上，权重参数为：</p>

<div>
$\Theta^{(1)} = 
\begin{bmatrix}
10 & -20 & -20
\end{bmatrix}$
</div>

<p>仅当$x_1$和$x_2$同时为0时，$h_{\Theta}(x) = 1$。</p>

<ol>
<li>$x_1\ OR\ x_2$</li>
</ol>

<p>模型同上，权重参数为：</p>

<div>
$\Theta^{(1)} = 
\begin{bmatrix}
-10 & 20 & 20
\end{bmatrix}$
</div>

<p>当$x_1$和$x_2$不同时为0时，$h_{\Theta}(x) = 1$。</p>

<ol>
<li>$x_1\ XNOR\ x_2$，$XNOR$为$NOT\ XOR$</li>
</ol>

<p>将前面得到的模型组合起来$x_1\ XNOR\ x_2 = (x_1\ AND\ x_2)\ OR\ (x_1\ XOR\ x_2)$</p>

<p>模型如下：</p>

<div>
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_1^{(2)} \newline
a_2^{(2)} 
\end{bmatrix} \rightarrow
\begin{bmatrix}
a^{(3)}
\end{bmatrix} \rightarrow
h_\Theta(x)
\end{align*}
$$
</div>

<p>第一层到第二层的$\Theta^{(1)}$分别表示$AND$和$NOR$操作：</p>

<div>
$$
\Theta^{(1)} = 
\begin{bmatrix}
-30 & 20 & 20 \newline
10 & -20 & -20
\end{bmatrix}
$$
</div>  

<p>第二层到第三层的$\Theta^{(2)}$表示$OR$操作：</p>

<div>
$$
\Theta^{(2)} = 
\begin{bmatrix}-10 & 20 & 20\end{bmatrix}
$$
</div>

<h1 id="多类分类问题:d35c8128725ed53542064246ed42d2c0">多类分类问题</h1>

<p>输出用向量来表示多类分类问题：</p>

<div>
$$
\begin{align*}
\begin{bmatrix}
x_0 \newline
x_1 \newline
x_2 \newline
\cdots \newline
x_n
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_0^{(2)} \newline
a_1^{(2)} \newline
a_2^{(2)} \newline
\cdots
\end{bmatrix} \rightarrow
\begin{bmatrix}
a_0^{(3)} \newline
a_1^{(3)} \newline
a_2^{(3)} \newline
\cdots
\end{bmatrix} \rightarrow \cdots \rightarrow
\begin{bmatrix}
h_\Theta(x)_1 \newline
h_\Theta(x)_2 \newline
h_\Theta(x)_3 \newline
h_\Theta(x)_4 \newline
\end{bmatrix} \rightarrow
\end{align*}
$$
</div>

<p>最后的结果：</p>

<div>
$$h_\Theta(x) = 
\begin{bmatrix}
0 \newline
0 \newline
1 \newline
0 \newline
\end{bmatrix}$$
</div>

<p>表示该样本属于第三类，$h_{\Theta}(x)_3$。</p>

      </article>


		
      
<div id="disqus_thread">
  <script type="text/javascript">
     var disqus_shortname = "dasheyuan";
     var disqus_identifier = "\/post\/2015-03-03-2";

     (function() {
       var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
       dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
       (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</div>


    </div>
  </div>
</div>
    <footer class="footer">
      <div class="p2 wrap">
        <div class="measure mt1 center">
      <nav class="social-icons icons">
<a class="fa fa-weibo weibo" href="http://weibo.com/dasheyuan"  target="_blank"></a>

<a class="fa fa-github github" href="https://github.com/dasheyuan"  target="_blank"></a>

</nav>

          <small>
            Copyright &#169; Chen Yuan 2016<br>
			Powered by <a href="http://gohugo.io/" target="_blank">Hugo</a> &amp; <a href="https://github.com/azmelanar/hugo-theme-pixyll" target="_blank">Pixyll.</a> 
            Hosted by  <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
		  </small>
        </div>
      </div>
    </footer>
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
    
 

    
    

	
</body>
</html>

