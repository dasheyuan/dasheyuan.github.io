<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>机器学习笔记3 有监督学习 分类 logistic回归 - Chen Yuan&#39;s Blogs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="keywords" content="">
  <link rel="canonical" href="http://dasheyuan.com/post/2015-02-12">

  
  

  
  

  
  

  <link rel="stylesheet" type="text/css" href="http://dasheyuan.com/css/combined-min.css">

</head>
<body class="">

<div class="site-wrap">
  <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://dasheyuan.com" class="site-title">Chen Yuan&#39;s Blogs</a>
      <nav class="site-nav right">
      <a href="/post">日志</a>
<a href="/tags">标签</a>
<a href="/about">关于</a>

</form>

      </nav>
      <div class="clearfix"></div>
    </div>
  </div>
</header>

  <div class="post p2 p-responsive wrap" role="main">
    <div class="measure">
      <div class="post-header mb2">
        <h3 class="py2">机器学习笔记3 有监督学习 分类 logistic回归</h3>
        <span class="post-meta">Feb 12, 2015 by </span><br>
        
      </div>
       <p class="post-meta">Tags:&nbsp;
        
            
            <a href="http://dasheyuan.com/tags/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0">机器学习</a>
        
            ,&nbsp;
            <a href="http://dasheyuan.com/tags/%e6%9c%89%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0">有监督学习</a>
        
            ,&nbsp;
            <a href="http://dasheyuan.com/tags/%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92">线性回归</a>
        
            ,&nbsp;
            <a href="http://dasheyuan.com/tags/logistic%e5%9b%9e%e5%bd%92">logistic回归</a>
        
            ,&nbsp;
            <a href="http://dasheyuan.com/tags/%e5%88%86%e7%b1%bb">分类</a>
        
      </p> 
      <article class="post-content">
      

<p>Andrew Ng cs229 Machine Learning 笔记</p>

<h1 id="分类问题:662363920bb15b8d466d06a6774c21df">分类问题</h1>

<p>分类问题和回归问题不同的是，分类问题的预测值$y$只能取离散值，而非连续值。首先来看一个二类分类问题，预测值$y$只能取0或1。0又被称作负例(negative class)，1被称作正例(positive class)。通常也用&rdquo;-&ldquo;,&rdquo;+&ldquo;符号来表示。对于一个样本集输入$x^{(i)}$，对应的目标值$y^{(i)}$也被为标注(lable)。</p>

<h2 id="logistic回归:662363920bb15b8d466d06a6774c21df">logistic回归</h2>

<p>也可以用线性回归的方法运用到分类问题上，但是这样做很容易得到不好的结果。稍微改变一下我们的假设函数$h_\theta(x)$，使其的取值在{0,1}范围内：</p>

<div>
$$h_\theta(x) = g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$$
$$g(z)=\frac1{1+e^{-z}}$$
</div>

<p>$g(z)$叫做logistic函数，也叫做sigmoid函数。$g(z)$的函数图像如下：</p>



<p>当$z\rightarrow \infty$时，$g(z)$趋近于1；当$z\rightarrow -\infty$时，$g(z)$趋近于0。因此$h(x)$的取值在0到1范围内。</p>

<p>求$g(z)$的导数可得：</p>

<div>
$$g'(z)=g(z)(1-g(z))$$
</div>

<p>下面是对分类问题作出的一些假设，预测函数$h_\theta(x)$将给出样本目标值分类为1的概率：</p>

<div>
$$P(y=1|x;\theta) = h_{\theta}(x)$$
$$P(y=0|x;\theta) = 1-h_{\theta}(x)$$
$$p(y|x;\theta) = (h_{\theta}(x)^y(1-h_{\theta}(x)^{1-y}))$$
</div>

<p>那么$\theta$的似然函数为：</p>

<div>
$$\begin{equation}
\begin{split}
L(\theta) =& p(\overrightarrow y|X;\theta)\\
=& \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=& \Pi_{i=1}^m (h_\theta(x^{(i)})^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}})
\end{split}
\end{equation}$$
</div>

<p>求log似然函数:</p>

<div>
$$l(\theta)=\log L(\theta)=\sum_{i=1}^m y^{(i)}\log h(x^{(i)})+(1-y^{(i)})\log (1-h(x^{(i)}))$$
</div>

<p>求最大似然估计，同样可以采用梯度下降的方法，更新$\theta$：</p>

<div>
$$\theta:=\theta+\alpha \nabla_\theta l(\theta)$$
</div>

<p>这里是求最大值，因此更新$\theta$是加上$l(\theta)$的偏导。</p>

<p>解之得到：</p>

<div>
$$\theta_j:=\theta_j+\alpha (y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$$
</div>

<p>这和之前在线性回归模型得到的LMS更新策略一样，这并不是巧合，而是因为线性回归和logistic回归都属于广义线性模型(GLM models)。</p>

<h2 id="perceptron学习算法:662363920bb15b8d466d06a6774c21df">perceptron学习算法</h2>

<p>有趣的是，如果这里不采用logistic函数，而是采用一种简单粗暴的只考虑阈值的函数g(z)：</p>

<div>
$$g(z) = \begin{cases}1,if z\geq 0\\0,if z < 0\end{cases}$$
</div>

<p>我们得到的更新$\theta$的策略和采用logistic函数得到的策略是一致。这种算法叫做感知器(perceptron)学习算法，感知器原指一种用来刻画大脑神经元的粗糙模型。虽然表面上看这种简单粗暴的方式和其他算法得到的结果是一样的，但是这是一种和logistic回归以及最小二乘线性回归非常不同的一类算法，它不能推导出有意义的概率解释，也不能通过极大似然估计得到。</p>

<h2 id="牛顿法:662363920bb15b8d466d06a6774c21df">牛顿法</h2>

<p>为了求$f(\theta)=0$时$\theta$的取值，牛顿法每次更新$\theta$：</p>

<div>
$$\theta:=\theta-\frac{f(\theta)}{f'(\theta)}$$
</div>

<p>要最大化似然函数$l(\theta)$的值，使其导数$l&rsquo;(\theta)＝0$。更新策略为：</p>

<div>
$$\theta:=\theta-\frac{l'(\theta)}{l''(\theta)}$$
</div>

<p>当$\theta$为向量时，推广更一般的牛顿法，这种方法也叫做牛顿－拉普森法(Newton-Raphson method)：</p>

<div>
$$\theta:=\theta-H^{-1} \nabla_\theta l(\theta)$$
</div>

<p>$\nabla_\theta l(\theta)$是$l(\theta)$对于$\theta$的偏导。$H$是$(n+1)*(n+1)$的矩阵，叫做Hessian：</p>

<div>
$$H_{ij}=\frac{\delta^2 l(\theta)}{\delta \theta_i \delta \theta_j}$$
</div>

<p>牛顿法收敛的速度通常比批量梯度下降要快，但是牛顿法每次迭代的计算量更大，每次迭代重新计算Hessian矩阵，需要$O(n^2)$的时间复杂度。但在n没有很大的情况下，牛顿法是更有效率的。将牛顿法用于logistic回归的log似然函数$l(\theta)$得到的方法也被称为Fisher scoring。</p>

      </article>


		
      
<div id="disqus_thread">
  <script type="text/javascript">
     var disqus_shortname = "dasheyuan";
     var disqus_identifier = "\/post\/2015-02-12";

     (function() {
       var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
       dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
       (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</div>


    </div>
  </div>
</div>
    <footer class="footer">
      <div class="p2 wrap">
        <div class="measure mt1 center">
      <nav class="social-icons icons">
<a class="fa fa-weibo weibo" href="http://weibo.com/dasheyuan"  target="_blank"></a>

<a class="fa fa-github github" href="https://github.com/dasheyuan"  target="_blank"></a>

</nav>

          <small>
            Copyright &#169; Chen Yuan 2016<br>
			Powered by <a href="http://gohugo.io/" target="_blank">Hugo</a> &amp; <a href="https://github.com/azmelanar/hugo-theme-pixyll" target="_blank">Pixyll.</a> 
            Hosted by  <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
		  </small>
        </div>
      </div>
    </footer>
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
    
 

    
    

	
</body>
</html>

