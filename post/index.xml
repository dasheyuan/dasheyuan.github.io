<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Posts on Chen Yuan&#39;s Blogs </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://dasheyuan.com/post/</link>
    <language>en-us</language>
    
    <copyright>Copyright (c) 2016,Chen Yuan; all rights reserved.</copyright>
    <updated>Mon, 31 Aug 2015 00:00:00 UTC</updated>
    
    <item>
      <title>Python代码风格指南（三）命名约定(PEP8中文翻译)</title>
      <link>http://dasheyuan.com/post/2015-09-14</link>
      <pubDate>Mon, 31 Aug 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-09-14</guid>
      <description>&lt;h1 id=&#34;命名约定-naming-conventions:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;命名约定（Naming Conventions）&lt;/h1&gt;

&lt;p&gt;Python标准库的命名约定有一些混乱，因此我们永远都无法保持一致。但如今仍然存在一些推荐的命名标准。新的模块和包（包括第三方框架）应该采用这些标准，但若是已经存在的包有另一套风格的话，还是应当与原有的风格保持内部一致。&lt;/p&gt;

&lt;h2 id=&#34;重写原则-overriding-principle:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;重写原则（Overriding Principle）&lt;/h2&gt;

&lt;p&gt;对于用户可见的公共部分API，其命名应当表达出功能用途而不是其具体的实现细节。&lt;/p&gt;

&lt;h2 id=&#34;描述性-命名风格-descriptive-naming-styles:26bc06cdd4e62ee943404b26c97620ec&#34;&gt;描述性：命名风格（Descriptive: Naming Styles）&lt;/h2&gt;

&lt;p&gt;存在很多不同的命名风格，最好能够独立地从命名对象的用途认出采用了哪种命名风格。&lt;/p&gt;

&lt;p&gt;以下是常用于区分的命名风格：&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python代码风格指南（二）字符串引用、空格、注释和版本注记(PEP8中文翻译)</title>
      <link>http://dasheyuan.com/post/2015-08-30</link>
      <pubDate>Sun, 30 Aug 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-08-30</guid>
      <description>&lt;h1 id=&#34;字符串引用-string-quotes:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;字符串引用(String Quotes)&lt;/h1&gt;

&lt;p&gt;在Python中表示字符串时，不管用单引号还是双引号都是一样的。但是不推荐将这两种方式看作一样并且混用。最好选择一种规则并坚持使用。当字符串中包含单引号时，采用双引号来表示字符串，反之也是一样，这样可以避免使用反斜杠，代码也更易读。&lt;/p&gt;

&lt;p&gt;对于三引号表示的字符串，使用双引号字符来表示（译注：即用&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;而不是&lt;code&gt;&#39;&#39;&#39;&lt;/code&gt;），这样可以和PEP 257的文档字符串（docstring）规则保持一致。&lt;/p&gt;

&lt;h1 id=&#34;表达式和语句中的空格-whitespace-in-expressions-and-statements:4f740b0dc79b8c0d748c37b6117fb209&#34;&gt;表达式和语句中的空格(Whitespace in Expressions and Statements)&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>最长回文子串Longest palindromic substring的四种算法</title>
      <link>http://dasheyuan.com/post/2015-08-11</link>
      <pubDate>Tue, 11 Aug 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-08-11</guid>
      <description>&lt;h2 id=&#34;题目描述:3c5494d961990eaf70786ecbf6c7dfc4&#34;&gt;题目描述：&lt;/h2&gt;

&lt;p&gt;给定字符串$S$，求其最长的回文子串。&lt;/p&gt;

&lt;p&gt;Leetcode:&lt;a href=&#34;https://leetcode.com/problems/longest-palindromic-substring/&#34;&gt;https://leetcode.com/problems/longest-palindromic-substring/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下面给出四种算法思路，分别是朴素枚举、动态规划、中心拓展和Manacher算法。&lt;/p&gt;

&lt;p&gt;其中，Manacher算法复杂度为$O(n)$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python代码风格指南（一）代码设计(PEP8中文翻译)</title>
      <link>http://dasheyuan.com/post/2015-07-04</link>
      <pubDate>Sat, 04 Jul 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-07-04</guid>
      <description>&lt;p&gt;翻译自：&lt;a href=&#34;https://www.python.org/dev/peps/pep-0008/&#34;&gt;PEP 8 - Style Guide for Python Code&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;介绍-introduction:0c8ec52d92a7c32bcce67dfaa3143f15&#34;&gt;介绍(Introduction)&lt;/h1&gt;

&lt;p&gt;这篇文档说明了Python主要发行版中标准库代码所遵守的规范。请参考实现Python的C代码风格指南信息PEP。&lt;/p&gt;

&lt;p&gt;这篇文档和PEP 257(Docstring Conventions)都改编自Guido(译注：Python之父)最早的Python风格指南文章，并加入了Barry风格指南里的内容。&lt;/p&gt;

&lt;p&gt;语言自身在发生着改变，随着新的规范的出现和旧规范的过时，代码风格也会随着时间演变。&lt;/p&gt;

&lt;p&gt;很多项目都有自己的一套风格指南。若和本指南有任何冲突，应该优先考虑其项目相关的那套指南。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习第一战——阿里天池移动推荐算法比赛经验总结攻略</title>
      <link>http://dasheyuan.com/post/2015-07-01</link>
      <pubDate>Wed, 01 Jul 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-07-01</guid>
      <description>&lt;p&gt;历时98天的&lt;a href=&#34;http://tianchi.aliyun.com/competition/introduction.htm?spm=5176.100066.333.5.0n5wpl&amp;amp;raceId=1&#34;&gt;阿里移动推荐算法&lt;/a&gt;终于结束了，有种终于下了贼船的感觉。总的来说，整个比赛的体验并不好：时间太长，资源不够，运气成分大，学到的干货少。虽然学到的干货不多，但这毕竟是在Data Science道路上进军的第一场实战，还是有必要好好总结一下。&lt;/p&gt;

&lt;h1 id=&#34;比赛统计:e22b26d8c6b3a4529619561860cf7538&#34;&gt;比赛统计&lt;/h1&gt;

&lt;p&gt;初赛名次：21&lt;/p&gt;

&lt;p&gt;复赛名次：22&lt;/p&gt;

&lt;p&gt;Python代码行数：2320&lt;/p&gt;

&lt;p&gt;SQL代码行数：6656&lt;/p&gt;

&lt;p&gt;天池平台数据表数：1006&lt;/p&gt;

&lt;p&gt;线下结果数：490&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记7 高偏差/低偏差，学习曲线，模型选择</title>
      <link>http://dasheyuan.com/post/2015-05-17</link>
      <pubDate>Sun, 17 May 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-05-17</guid>
      <description>&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;p&gt;原文：&lt;a href=&#34;https://share.coursera.org/wiki/index.php/ML:Advice_for_Applying_Machine_Learning&#34;&gt;https://share.coursera.org/wiki/index.php/ML:Advice_for_Applying_Machine_Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;面对一个机器学习问题，我们提取好特征，挑选好训练集，选择一种机器学习算法，然后学习预测得到了第一步结果。然而我们不幸地发现，在测试集上的准确率低得离谱，误差高得吓人，要提高准确率、减少误差的话，下一步该做些什么呢？&lt;/p&gt;

&lt;p&gt;可以采用以下的方法来减少预测的误差：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获得更多的训练样本&lt;/li&gt;
&lt;li&gt;减少特征的数量&lt;/li&gt;
&lt;li&gt;增加特征的数量&lt;/li&gt;
&lt;li&gt;使用多项式特征&lt;/li&gt;
&lt;li&gt;增大或减小正则化参数$\lambda$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但不要盲目在这些可行的方法里随便选一种来提升模型，需要用一些诊断模型的技术来帮助我们选择使用哪种策略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记5 神经网络2 参数学习</title>
      <link>http://dasheyuan.com/post/2015-03-26</link>
      <pubDate>Thu, 26 Mar 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-03-26</guid>
      <description>&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;成本函数-cost-function:8735cce9d7fd270391f5ec199a69102a&#34;&gt;成本函数(Cost Function)&lt;/h1&gt;

&lt;p&gt;以下是我们会用到的一些变量：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$L$表示神经网络的层数&lt;/li&gt;
&lt;li&gt;$s_l$表示第$l$层的神经单元数(不包括偏差(bias)单元)&lt;/li&gt;
&lt;li&gt;$K$表示输出单元数(分类数)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当有多个输出类别时，采用$h_\Theta(x)_k$表示第$k$个输出的假设结果。&lt;/p&gt;

&lt;p&gt;神经网络的成本函数是logistic回归中的成本函数更普遍的一种形式。&lt;/p&gt;

&lt;p&gt;logistic回归中的成本函数为：&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>python numpy 计算自相关系数</title>
      <link>http://dasheyuan.com/post/2015-03-15</link>
      <pubDate>Sun, 15 Mar 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-03-15</guid>
      <description>&lt;p&gt;在分析时间序列时，通常需要计算一个序列的自相关系数。自相关(&lt;a href=&#34;http://en.wikipedia.org/wiki/Autocorrelation&#34;&gt;Autocorrelation&lt;/a&gt;)又叫做序列相关，通常采用自相关系数来发现序列的重复规律，周期等信息。&lt;/p&gt;

&lt;p&gt;我们有序列$X:x_1,x_2,x_3,&amp;hellip;,x_n$，设$X_{s,t}$为$s$时刻开始，$t$时刻结束的序列：$x_s,x_{s+1}&amp;hellip;,x_{t-1},x_t$。$\mu_{s,t}$为序列$X_{s,t}$的均值，$\sigma_{s,t}$为序列$X_{s,t}$的标准差。那么一阶自相关系数为：&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记5 神经网络1 模型表达</title>
      <link>http://dasheyuan.com/post/2015-03-03-2</link>
      <pubDate>Tue, 03 Mar 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-03-03-2</guid>
      <description>&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;神经网络:d35c8128725ed53542064246ed42d2c0&#34;&gt;神经网络&lt;/h1&gt;

&lt;h2 id=&#34;非线性假设:d35c8128725ed53542064246ed42d2c0&#34;&gt;非线性假设&lt;/h2&gt;

&lt;p&gt;在特征变量数较大的情况下，采用线性回归会很难处理，比如我的数据集有3个特征变量，想要在假设中引入所有特征变量的平方项：&lt;/p&gt;

&lt;div&gt;
$$g(\theta_0 + \theta_1x_1^2 + \theta_2x_1x_2 + \theta_3x_1x_3  + \theta_4x_2^2 + \theta_5x_2x_3  + \theta_6x_3^2 )$$
&lt;/div&gt;

&lt;p&gt;共有6个特征，假设我们想知道选取其中任意两个可重复的平方项有多少组合，采用允许重复的组合公式计算$\frac{(n+r-1)!}{r!(n-1)!}$，共有$\frac{(3 + 2 - 1)!}{(2!\cdot (3-1)!)} = 6$种特征变量的组合。对于100个特征变量，则共有$\frac{(100 + 2 - 1)!}{(2\cdot (100-1)!)} = 5050$个新的特征变量。&lt;/p&gt;

&lt;p&gt;可以大致估计特征变量的平方项组合个数的增长速度为$\mathcal{O}(\frac{n^2}2)$，立方项的组合个数的增长为$\mathcal{O}(n^3)$。这些增长都十分陡峭，让实际问题变得很棘手。&lt;/p&gt;

&lt;p&gt;在变量假设十分复杂的情况下，神经网络提供了另一种机器学习算法。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记4 正则化</title>
      <link>http://dasheyuan.com/post/2015-02-17</link>
      <pubDate>Tue, 17 Feb 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-02-17</guid>
      <description>&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;正则化-regularization:e5cda6b6e7686a83e2d03b1aaf88c83b&#34;&gt;正则化 Regularization&lt;/h1&gt;

&lt;p&gt;为了和正规方程(normal equation)里&amp;rdquo;正规&amp;rdquo;区分开来，这里Regularization都译作“正则化”，有些地方也用的是“正规化”。以下内容来自&lt;a href=&#34;http://en.wikipedia.org/w/index.php?title=Regularization_(mathematics&#34;&gt;wikipedia&lt;/a&gt;)：&lt;/p&gt;

&lt;p&gt;正则化是指通过引入额外新信息来解决机器学习中过拟合问题的一种方法。这种额外信息通常的形式是模型复杂性带来的惩罚度。正则化的一种理论解释是它试图引入&lt;a href=&#34;http://en.wikipedia.org/wiki/Occam%27s_razor&#34;&gt;奥卡姆剃刀原则&lt;/a&gt;。而从贝叶斯的观点来看，正则化则是在模型参数上引入了某种先验的分布。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记3 有监督学习 分类 logistic回归</title>
      <link>http://dasheyuan.com/post/2015-02-12</link>
      <pubDate>Thu, 12 Feb 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-02-12</guid>
      <description>&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;分类问题:662363920bb15b8d466d06a6774c21df&#34;&gt;分类问题&lt;/h1&gt;

&lt;p&gt;分类问题和回归问题不同的是，分类问题的预测值$y$只能取离散值，而非连续值。首先来看一个二类分类问题，预测值$y$只能取0或1。0又被称作负例(negative class)，1被称作正例(positive class)。通常也用&amp;rdquo;-&amp;ldquo;,&amp;rdquo;+&amp;ldquo;符号来表示。对于一个样本集输入$x^{(i)}$，对应的目标值$y^{(i)}$也被为标注(lable)。&lt;/p&gt;

&lt;h2 id=&#34;logistic回归:662363920bb15b8d466d06a6774c21df&#34;&gt;logistic回归&lt;/h2&gt;

&lt;p&gt;也可以用线性回归的方法运用到分类问题上，但是这样做很容易得到不好的结果。稍微改变一下我们的假设函数$h_\theta(x)$，使其的取值在{0,1}范围内：&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释</title>
      <link>http://dasheyuan.com/post/2015-02-11</link>
      <pubDate>Wed, 11 Feb 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-02-11</guid>
      <description>&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;有监督学习:a41c550b5852f048f913986db76cdf89&#34;&gt;有监督学习&lt;/h1&gt;

&lt;h2 id=&#34;局部加权线性回归-locally-weighted-linear-regression:a41c550b5852f048f913986db76cdf89&#34;&gt;局部加权线性回归(Locally weighted linear regression)&lt;/h2&gt;

&lt;p&gt;参数学习算法(parametric learning algorithm)：参数个数固定&lt;/p&gt;

&lt;p&gt;非参数学习算法(non-parametric learning algorithm)：参数个数随样本增加&lt;/p&gt;

&lt;p&gt;特征选择对参数学习算法非常重要，否则会出现下面的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欠拟合(underfitting)：特征过少，模型过于简单，高偏差(high bias)，不能很好拟合训练集&lt;/li&gt;
&lt;li&gt;过拟合(overfitting)：特征过多，模型过于复杂，高方差(high variance)，过于拟合训练集，不能很好预测新样本&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于非参数学习算法来说，并不需要进行精心的特征选择，局部加权线性回归就是这样。&lt;/p&gt;

&lt;p&gt;局部加权回归又叫做Loess，其成本函数为：&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习笔记1 有监督学习 线性回归 LMS算法 正规方程</title>
      <link>http://dasheyuan.com/post/2015-02-10</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-02-10</guid>
      <description>&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;有监督学习:b33d274e87081502d65882ed2d51cd57&#34;&gt;有监督学习&lt;/h1&gt;

&lt;p&gt;先理清几个概念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$x^{(i)}$表示&amp;rdquo;输入&amp;rdquo;变量(&amp;ldquo;input&amp;rdquo; variables)，也称为特征值(features)。&lt;/li&gt;
&lt;li&gt;$y^{(i)}$表示&amp;rdquo;输出&amp;rdquo;变量(&amp;ldquo;output&amp;rdquo; variables)，也称为目标值(target)。&lt;/li&gt;
&lt;li&gt;一对$(x^{(i)},y^{(i)})$称为一个训练样本(training example)，用作训练的数据集就是就是一组$m$个训练样本${(x^{(i)},y^{(i)});i=1,&amp;hellip;,m}$，被称为训练集(training set)。&lt;/li&gt;
&lt;li&gt;$X$表示输入变量的取值空间，$Y$表示输出变量的取值空间。那么$h:X \rightarrow Y$是训练得到的映射函数，对于每个取值空间X的取值，都能给出取值空间Y上的一个预测值。函数$h$的含义为假设(hypothesis)。&lt;/li&gt;
&lt;li&gt;图形化表示整个过程：&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hugo静态网站生成器中文教程</title>
      <link>http://dasheyuan.com/post/2015-01-31</link>
      <pubDate>Sat, 31 Jan 2015 00:30:03 CST</pubDate>
      
      <guid>http://dasheyuan.com/post/2015-01-31</guid>
      <description>&lt;h1 id=&#34;前言:d605f9890f3528aea462ac7515ece633&#34;&gt;前言&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://gohugo.io&#34;&gt;Hugo&lt;/a&gt;是什么？官方文档是这样介绍它的：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hugo is a general-purpose website framework. Technically speaking, Hugo is a static site generator.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hugo是一种通用的网站框架。严格来说，Hugo应该被称作静态网站生成器。&lt;/p&gt;

&lt;p&gt;静态网站生成器从字面上来理解，就是将你的内容生成静态网站。所谓“静态”的含义其实反映在网站页面的生成的时间。一般的web服务器（WordPress, Ghost, Drupal等等）在收到页面请求时，需要调用数据库生成页面（也就是HTML代码），再返回给用户请求。而静态网站则不需要在收到请求后生成页面，而是在整个网站建立起之前就将所有的页面全部生成完成，页面一经生成便称为静态文件，访问时直接返回现成的静态页面，不需要数据库的参与。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scikit-Learn机器学习介绍（中文翻译）</title>
      <link>http://dasheyuan.com/post/2014-12-02</link>
      <pubDate>Tue, 02 Dec 2014 00:00:00 UTC</pubDate>
      
      <guid>http://dasheyuan.com/post/2014-12-02</guid>
      <description>&lt;p&gt;翻译自：&lt;a href=&#34;http://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34;&gt;http://scikit-learn.org/stable/tutorial/basic/tutorial.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;以后可能会根据自己的学习慢慢翻译其他的章节，水平有限，不足之处请指正。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本章内容
在本章中，我们会介绍在使用scikit-learn中遇到的[机器学习]&lt;a href=&#34;machine learning&#34;&gt;1&lt;/a&gt;术语，以及一个简单的机器学习例子。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;机器学习-问题设定:935e1a74f960fd04b26b502f6058f057&#34;&gt;机器学习：问题设定&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;一般来说，机器学习问题可以这样来理解：我们有n个[样本]&lt;a href=&#34;sample&#34;&gt;2&lt;/a&gt;的数据集，想要预测未知数据的属性。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

